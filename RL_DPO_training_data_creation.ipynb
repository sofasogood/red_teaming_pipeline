{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHHo2W3ql7dQ"
      },
      "outputs": [],
      "source": [
        "!pip install unsloth\n",
        "!pip install datasets\n",
        "from datasets import load_dataset, Dataset\n",
        "import pandas as pd\n",
        "import re\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from typing import List, Dict, Optional\n",
        "import wandb\n",
        "from dataclasses import dataclass\n",
        "import random\n",
        "import time\n",
        "import pickle\n",
        "import requests\n",
        "## RL specific\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKhs6Ifrl_HT",
        "outputId": "b1bfb303-5154-4202-96ab-3eb96d521a46"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): MistralForCausalLM(\n",
              "      (model): MistralModel(\n",
              "        (embed_tokens): Embedding(32768, 4096, padding_idx=770)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x MistralDecoderLayer(\n",
              "            (self_attn): MistralAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "            )\n",
              "            (mlp): MistralMLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
              "        (rotary_emb): MistralRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "\n",
        "# Configure quantization for inference\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# Load base model with quantization\n",
        "base_model_name = \"unsloth/mistral-7b-v0.3-bnb-4bit\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "# Load LoRA adapter weights\n",
        "adapter_path = \"/content/drive/MyDrive/models/ai_safety/finetuned_model_0131/mistral7b_finetuned/checkpoints/checkpoint-505\"\n",
        "model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    adapter_path,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Optional: Merge LoRA weights with base model for faster inference\n",
        "# model = model.merge_and_unload()\n",
        "\n",
        "# Set eval mode\n",
        "model.eval()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing red_teaming_pipeline\n",
        "!git clone https://github.com/sofasogood/red_teaming_pipeline.git\n",
        "%cd red_teaming_pipeline\n",
        "!pip install -e .\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "cTkqRp-0f5PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qCRAwh-e5j7E"
      },
      "outputs": [],
      "source": [
        "from dataset_creation.data_processing_functions.data_creation import extract_question\n",
        "from dataset_creation.data_processing_functions.data_creation_rl import build_instruct_prompt, generate_response,AIJudge, JudgementChoice, JudgementError\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "e9EauyC6_-A4"
      },
      "outputs": [],
      "source": [
        "OPENROUTER_API_URL = \"https://openrouter.ai/api/v1/completions\"\n",
        "OPENROUTER_API_KEY = \"insert your key here\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wMHla30N5lOf"
      },
      "outputs": [],
      "source": [
        "#Load dataset\n",
        "dataset_red_team = load_dataset(\"Anthropic/hh-rlhf\", data_dir = \"red-team-attempts\")\n",
        "dataset_red_team_train = dataset_red_team[\"train\"].to_pandas()\n",
        "dataset_red_team_train[\"question\"] = dataset_red_team_train[\"transcript\"].apply(extract_question)\n",
        "dataset_red_team_train_subset = dataset_red_team_train[dataset_red_team_train[\"task_descripton_harmlessness_score\"]<0.8].sort_values(by = \"task_descripton_harmlessness_score\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ai_feedback_data = []  #store pairs + preference\n"
      ],
      "metadata": {
        "id": "CSxcbSTDrmk0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ckHnHluK6284"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "def save_to_csv():\n",
        "  DIR = \"/content/drive/MyDrive/AI_SAFETY/\"\n",
        "  # Convert to DataFrame\n",
        "  df_llm_judge_2 = pd.DataFrame(ai_feedback_data)\n",
        "  start_idx = 0\n",
        "  end_idx = 0\n",
        "  if ai_feedback_data:\n",
        "    start_idx = ai_feedback_data[0][\"idx\"]\n",
        "    end_idx = ai_feedback_data[-1][\"idx\"]\n",
        "  df_llm_judge_2.to_csv(f\"{DIR}df_llm_judge_{start_idx}_{end_idx}.csv\")\n",
        "\n",
        "save_to_csv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeN-AOA1CK2m",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "harmful_prompts = [x for x in dataset_red_team_train[\"question\"]]\n",
        "\n",
        "for idx, prompt in enumerate(harmful_prompts):\n",
        "    already_done = [entry[\"idx\"] for entry in ai_feedback_data]\n",
        "    if idx not in already_done:\n",
        "      start_time_all = time.time()\n",
        "      print(f\"Prompt {idx}: {prompt}\", flush=True)\n",
        "      print(\"---\", flush=True)\n",
        "      start_time_a = time.time()\n",
        "      responseA = generate_response(prompt=prompt, model=model, tokenizer=tokenizer, use_local_model=True)\n",
        "      time_taken_a = time.time() - start_time_a\n",
        "      print(f\"  Response A >> {responseA}\", flush=True)\n",
        "      print(f\"    Time taken: {time_taken_a} seconds\", flush=True)\n",
        "      print(\"---\", flush=True)\n",
        "      start_time_b = time.time()\n",
        "      responseB = generate_response(prompt=prompt, model=model, tokenizer=tokenizer, use_local_model=True)\n",
        "      time_taken_b = time.time() - start_time_b\n",
        "      print(f\"  Response B >> {responseB}\", flush=True)\n",
        "      print(f\"    Time taken: {time_taken_b} seconds\", flush=True)\n",
        "      print(\"---\", flush=True)\n",
        "\n",
        "      start_time_winner = time.time()\n",
        "      judge = AIJudge(api_key=OPENROUTER_API_KEY)\n",
        "      try:\n",
        "        winner = judge.judge(prompt, responseA, responseB)\n",
        "      except Exception as e:\n",
        "        print(f\"Error in ai_judge: {str(e)}\")\n",
        "        winner = JudgementChoice.INVALID\n",
        "      end_time_winner = time.time()\n",
        "      print(f\"Winner: {winner}\", flush=True)\n",
        "      print(f\"    Time taken: {end_time_winner - start_time_winner} seconds\", flush=True)\n",
        "      end_time = time.time()\n",
        "      print(f\"Total time taken: {end_time - start_time_all} seconds\", flush=True)\n",
        "      print(\"========\", flush=True)\n",
        "\n",
        "      if idx not in ai_feedback_data:\n",
        "        ai_feedback_data.append({\n",
        "            \"idx\":idx,\n",
        "            \"prompt\": prompt,\n",
        "            \"responseA\": responseA,\n",
        "            \"responseB\": responseB,\n",
        "            \"winner\": winner\n",
        "        })\n",
        "      else:\n",
        "        continue\n",
        "\n",
        "      if idx % 50 == 0:\n",
        "        save_to_csv()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPHKker/q1BozNfRskpmE9s"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}