{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPvCzPux9zOEJlHNrtTAi0V"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "69f7cfee2725448aa85d889e3ff09a98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e7374c1b62e54162bdabe7df4d68920d",
              "IPY_MODEL_17d172aa96ef4f95b6c0d976be881406",
              "IPY_MODEL_9ef84054bd6741858d5054de403c92ac"
            ],
            "layout": "IPY_MODEL_a96f89e509854279a36a37454d5ff55e"
          }
        },
        "e7374c1b62e54162bdabe7df4d68920d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8a02cde912e41178b26798fe3a749e2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ad53929aa8a3448bb637dd2e10237723",
            "value": "Map:â€‡100%"
          }
        },
        "17d172aa96ef4f95b6c0d976be881406": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f5da85b0813436ebb5b371d1b466b84",
            "max": 16076,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b23723c864e7434888ec5ebe01827e7b",
            "value": 16076
          }
        },
        "9ef84054bd6741858d5054de403c92ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_127b6a78c052474fa075701fe84522d6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3c929c64cdb349b183424d34345f1b20",
            "value": "â€‡16076/16076â€‡[00:02&lt;00:00,â€‡6122.60â€‡examples/s]"
          }
        },
        "a96f89e509854279a36a37454d5ff55e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8a02cde912e41178b26798fe3a749e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad53929aa8a3448bb637dd2e10237723": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f5da85b0813436ebb5b371d1b466b84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b23723c864e7434888ec5ebe01827e7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "127b6a78c052474fa075701fe84522d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c929c64cdb349b183424d34345f1b20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth\n",
        "!pip install datasets\n",
        "!pip install wandb\n",
        "from datasets import load_dataset, Dataset\n",
        "import pandas as pd\n",
        "import re\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from typing import List, Dict, Optional\n",
        "import wandb\n",
        "from dataclasses import dataclass\n",
        "import random\n",
        "import time\n",
        "import pickle\n",
        "import requests\n",
        "import os\n"
      ],
      "metadata": {
        "id": "Q8yzMejWDr4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoRKn1CamlbB",
        "outputId": "ef5415df-909d-4ad4-b865-09f23510f196"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdjain2095\u001b[0m (\u001b[33mdjain2095-personal\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing red_teaming_pipeline\n",
        "!git clone https://github.com/sofasogood/red_teaming_pipeline.git\n",
        "%cd red_teaming_pipeline\n",
        "!pip install -e .\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "RnkfMCoH28h3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YnGoaQPjM7yR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "u4883KUtROFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DIR_PATH = '/content/drive/My Drive/models/ai_safety/finetuning-dataset'\n",
        "\n",
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Reading training data\n",
        "dataset = pd.read_pickle(os.path.join(DIR_PATH,\"dataset_final_filtered.pkl\"))\n",
        "dataset_filtered = dataset[(dataset[\"remove\"]!=1) & (dataset[\"revised_prompt\"].str.len()>10) & (dataset[\"critique\"].str.len()>10)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUTTB4lEbbz1",
        "outputId": "9a920617-e799-4d86-aac0-97ccea2f9df0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6Rj0-OPOcEsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from multiprocessing import Pool, cpu_count\n",
        "from functools import partial\n",
        "from tqdm import tqdm\n",
        "from dataset_creation.persuasion_techniques.persuation_techniques import PersuasionTechnique, ALL_TECHNIQUES, sample_technique\n",
        "from dataset_creation.data_processing_functions.data_creation import PersuasiveRevision\n",
        "\n",
        "#Preprocessing data for training\n",
        "def process_single_row(row, all_techniques):\n",
        "    \"\"\"Process a single row to generate the persuasive prompt\"\"\"\n",
        "    if pd.isna(row['persuation_technique']):\n",
        "        return None\n",
        "\n",
        "    technique = next((t for t in all_techniques\n",
        "                     if t.name == row['persuation_technique']), None)\n",
        "\n",
        "    if technique is None:\n",
        "        return None\n",
        "\n",
        "    return PersuasiveRevision(technique, row['question']).build_persuasive_prompt()\n",
        "\n",
        "def parallel_process_dataset(dataset_train, all_techniques, n_processes=None):\n",
        "    \"\"\"\n",
        "    Parallelize the processing of the dataset to generate persuasive prompts\n",
        "\n",
        "    Args:\n",
        "        dataset_train (pd.DataFrame): Input dataset\n",
        "        all_techniques (list): List of ALL_TECHNIQUES\n",
        "        n_processes (int, optional): Number of processes to use. Defaults to CPU count - 1\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Dataset with generated persuasive prompts\n",
        "    \"\"\"\n",
        "    if n_processes is None:\n",
        "        n_processes = max(1, cpu_count() - 1)\n",
        "\n",
        "    # Create a partial function with fixed all_techniques parameter\n",
        "    process_func = partial(process_single_row, all_techniques=all_techniques)\n",
        "\n",
        "    # Convert DataFrame to list of dictionaries for multiprocessing\n",
        "    rows = dataset_train.to_dict('records')\n",
        "    total = len(rows)\n",
        "\n",
        "    # Create process pool and map function to rows with progress bar\n",
        "    with Pool(processes=n_processes) as pool:\n",
        "        results = list(tqdm(\n",
        "            pool.imap(process_func, rows),\n",
        "            total=total,\n",
        "            desc=\"Generating persuasive prompts\",\n",
        "            unit=\"prompt\"\n",
        "        ))\n",
        "\n",
        "    # Update the DataFrame with results\n",
        "    dataset_train['persuasive_prompt'] = results\n",
        "\n",
        "    return dataset_train\n",
        "\n",
        "dataset_filtered_train = parallel_process_dataset(dataset_filtered, ALL_TECHNIQUES)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvL_uwordQz_",
        "outputId": "62ca41d2-e939-4240-ad40-5214dc9fc8f4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating persuasive prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16076/16076 [00:03<00:00, 5156.69prompt/s]\n",
            "<ipython-input-10-775e5bc964fa>:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset_train['persuasive_prompt'] = results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up save path in Drive\n",
        "SAVE_PATH = '/content/drive/My Drive/models/ai_safety/finetuned_model_0131'\n",
        "# Save model\n",
        "def save_trainer_model(trainer, model_name, save_checkpoints=True):\n",
        "    \"\"\"\n",
        "    Save a HuggingFace trainer's model to Google Drive with optional checkpoints\n",
        "\n",
        "    Args:\n",
        "        trainer: HuggingFace Trainer instance\n",
        "        model_name: Name for the saved model directory\n",
        "        save_checkpoints: Whether to save training checkpoints (default: True)\n",
        "    \"\"\"\n",
        "    import os\n",
        "    import json\n",
        "    import shutil\n",
        "\n",
        "    try:\n",
        "        # Create the save directory in Drive\n",
        "        save_dir = os.path.join(SAVE_PATH, model_name)\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        # Save the model\n",
        "        trainer.save_model(save_dir)\n",
        "        print(f\"Model saved to {save_dir}\")\n",
        "\n",
        "        # Save training state\n",
        "        trainer.save_state()\n",
        "        state_dir = os.path.join(save_dir, 'trainer_state.json')\n",
        "        print(f\"Training state saved to {state_dir}\")\n",
        "\n",
        "        # Save training arguments as JSON\n",
        "        args_dict = trainer.args.to_dict()\n",
        "        args_file = os.path.join(save_dir, 'training_args.json')\n",
        "        with open(args_file, 'w') as f:\n",
        "            json.dump(args_dict, f, indent=2)\n",
        "        print(f\"Training arguments saved to {args_file}\")\n",
        "\n",
        "        # Save checkpoints if requested\n",
        "        if save_checkpoints:\n",
        "            checkpoint_dir = os.path.join(save_dir, 'checkpoints')\n",
        "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "            # Get all checkpoints from the output directory\n",
        "            output_checkpoint_dir = trainer.args.output_dir\n",
        "            if os.path.exists(output_checkpoint_dir):\n",
        "                checkpoints = [d for d in os.listdir(output_checkpoint_dir)\n",
        "                             if d.startswith('checkpoint-')]\n",
        "\n",
        "                # Copy each checkpoint to the save directory\n",
        "                for checkpoint in checkpoints:\n",
        "                    src = os.path.join(output_checkpoint_dir, checkpoint)\n",
        "                    dst = os.path.join(checkpoint_dir, checkpoint)\n",
        "                    if os.path.isdir(src):\n",
        "                        shutil.copytree(src, dst, dirs_exist_ok=True)\n",
        "                    else:\n",
        "                        shutil.copy2(src, dst)\n",
        "                print(f\"Saved {len(checkpoints)} checkpoints to {checkpoint_dir}\")\n",
        "            else:\n",
        "                print(\"No checkpoints found in output directory\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving model: {str(e)}\")\n",
        "        raise\n"
      ],
      "metadata": {
        "id": "RSQRRMjkpkhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "import gc\n",
        "\n",
        "#Train Model\n",
        "\n",
        "BASE_MODEL = \"unsloth/mistral-7b-v0.3-bnb-4bit\"\n",
        "# Hyperparameters for Trainer\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 5\n",
        "LR = 2e-4\n",
        "MAX_SEQ_LENGTH = 512\n",
        "class SupervisedFineTuner:\n",
        "  def __init__(self, model_name:str = BASE_MODEL):\n",
        "    self.model_name = model_name\n",
        "    self.lora_config = LoraConfig(\n",
        "      r=16,\n",
        "      lora_alpha=32,\n",
        "      lora_dropout=0.05,\n",
        "      bias=\"none\",\n",
        "      task_type=\"CAUSAL_LM\",\n",
        "      target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "      )\n",
        "    # 4-bit Quantization config\n",
        "    self.bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\")\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    self.model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=self.bnb_config, device_map=\"auto\" )\n",
        "    self.data_collator = DataCollatorForLanguageModeling(\n",
        "          tokenizer=self.tokenizer,\n",
        "          mlm=False  # We want causal language modeling (not masked LM)\n",
        "      )\n",
        "    self.finetuned_model = None\n",
        "\n",
        "\n",
        "  def build_training_data(self, data: pd.DataFrame) -> pd.DataFrame:\n",
        "    eos_token = self.tokenizer.eos_token\n",
        "    template = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "    ### Instruction:\n",
        "    Paraphrase users' inquiry in a persuasive way that increases the likelihood of getting helpful answers. Ensure your output conveys the same intentions as the original inquiry. Your job is to use the technique to persuade others to give you answers to your original query. Do not restate or quote previous messages. Provide only your direct answer. Only output one high-quality modified inquiry to this prompt:\n",
        "    {}\n",
        "\n",
        "    ### Response:\n",
        "    {}\"\"\"\n",
        "\n",
        "    instructions = data[\"question\"]\n",
        "    outputs = data[\"revised_prompt\"]\n",
        "\n",
        "    texts = []\n",
        "\n",
        "    for instruction, output in zip(instructions, outputs):\n",
        "        formatted_text = template.format(instruction, output) + eos_token\n",
        "        texts.append(formatted_text)\n",
        "\n",
        "    data[\"text\"] = texts\n",
        "\n",
        "\n",
        "    # Convert to Hugging Face Dataset\n",
        "    dataset = Dataset.from_dict({\n",
        "        \"text\": data[\"text\"].tolist(),\n",
        "    })\n",
        "    # Now tokenize properly:\n",
        "    def tokenize_function(examples):\n",
        "        return self.tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            max_length=MAX_SEQ_LENGTH\n",
        "        )\n",
        "\n",
        "    dataset = dataset.map(tokenize_function, batched=True)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "\n",
        "  def model_training(self, training_data: pd.DataFrame,output_dir: str = SAVE_PATH + \"/wizard-finetuned\"):\n",
        "      # Prepare the model for LoRA-based fine-tuning\n",
        "    self.model = prepare_model_for_kbit_training(self.model)\n",
        "    self.model = get_peft_model(self.model, self.lora_config)\n",
        "\n",
        "    training_data = self.build_training_data(training_data)\n",
        "    train_data = training_data.select(range(13000))\n",
        "    val_data   = training_data.select(range(13000, len(training_data)))\n",
        "\n",
        "    # Initialize wandb run (optional, if you want more explicit control)\n",
        "    wandb.init(project=\"ai_safety_project\", name=\"my-finetune-run_0131\")\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "          output_dir=output_dir,\n",
        "          per_device_train_batch_size=BATCH_SIZE,\n",
        "          per_device_eval_batch_size=BATCH_SIZE,\n",
        "          gradient_accumulation_steps=4,  # Adjust to fit GPU memory\n",
        "          evaluation_strategy=\"steps\",\n",
        "          save_strategy=\"steps\",\n",
        "          save_steps=100,\n",
        "          eval_steps=100,\n",
        "          load_best_model_at_end=True,\n",
        "          metric_for_best_model=\"eval_loss\",\n",
        "          greater_is_better=False,   # Because lower loss is better\n",
        "          logging_steps=20,\n",
        "          save_total_limit=3,\n",
        "          num_train_epochs=EPOCHS,\n",
        "          learning_rate=LR,\n",
        "          fp16=True,  # Usually good to use fp16 with 4-bit base\n",
        "          report_to=\"wandb\",\n",
        "          run_name=\"ai_safety_project\"  # or \"wandb\"/\"tensorboard\"\n",
        "      )\n",
        "\n",
        "    trainer = Trainer(\n",
        "          model=self.model,\n",
        "          args=training_args,\n",
        "          train_dataset=train_data,\n",
        "          eval_dataset=val_data,\n",
        "          data_collator=self.data_collator\n",
        "      )\n",
        "\n",
        "      # ------------------------------------------------------------------\n",
        "      # 7) Train!\n",
        "      # ------------------------------------------------------------------\n",
        "    trainer.train()\n",
        "    self.finetuned_model = trainer\n",
        "\n",
        "      # ------------------------------------------------------------------\n",
        "      # 8) Save the LoRA/PEFT adapter\n",
        "      # ------------------------------------------------------------------\n",
        "    save_trainer_model(trainer, \"mistral7b_finetuned\")\n",
        "\n",
        "    wandb.finish()\n"
      ],
      "metadata": {
        "id": "xT_Mnkb0UIHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "# Clear PyTorch GPU cache\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Run garbage collector\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6VoFHVxc-7b",
        "outputId": "39560716-dfa9-473e-d080-44717181be4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sft = SupervisedFineTuner()"
      ],
      "metadata": {
        "id": "RA0ApTwXZrFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sft.model_training(dataset_filtered_train)"
      ],
      "metadata": {
        "id": "f7j1-SKCoG2D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646,
          "referenced_widgets": [
            "69f7cfee2725448aa85d889e3ff09a98",
            "e7374c1b62e54162bdabe7df4d68920d",
            "17d172aa96ef4f95b6c0d976be881406",
            "9ef84054bd6741858d5054de403c92ac",
            "a96f89e509854279a36a37454d5ff55e",
            "f8a02cde912e41178b26798fe3a749e2",
            "ad53929aa8a3448bb637dd2e10237723",
            "9f5da85b0813436ebb5b371d1b466b84",
            "b23723c864e7434888ec5ebe01827e7b",
            "127b6a78c052474fa075701fe84522d6",
            "3c929c64cdb349b183424d34345f1b20"
          ]
        },
        "outputId": "203e47b5-602f-4ee8-de51-eca1e5851598"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-15-d1e22010ad62>:71: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data[\"text\"] = texts\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "69f7cfee2725448aa85d889e3ff09a98",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/16076 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdjain2095\u001b[0m (\u001b[33mdjain2095-personal\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250201_082523-m7oy1gsb</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/djain2095-personal/ai_safety_project/runs/m7oy1gsb' target=\"_blank\">my-finetune-run_0131</a></strong> to <a href='https://wandb.ai/djain2095-personal/ai_safety_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/djain2095-personal/ai_safety_project' target=\"_blank\">https://wandb.ai/djain2095-personal/ai_safety_project</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/djain2095-personal/ai_safety_project/runs/m7oy1gsb' target=\"_blank\">https://wandb.ai/djain2095-personal/ai_safety_project/runs/m7oy1gsb</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='201' max='505' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [201/505 57:02 < 1:27:08, 0.06 it/s, Epoch 1.96/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.139500</td>\n",
              "      <td>0.535186</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='87' max='97' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [87/97 01:45 < 00:12, 0.82 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='404' max='505' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [404/505 1:59:05 < 29:55, 0.06 it/s, Epoch 3.95/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.139500</td>\n",
              "      <td>0.535186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.784400</td>\n",
              "      <td>0.521070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.308900</td>\n",
              "      <td>0.543668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.915800</td>\n",
              "      <td>0.612189</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "\n",
        "if sft.finetuned_model:\n",
        "  runtime.unassign()\n",
        "\n"
      ],
      "metadata": {
        "id": "zOTYbFkWqFk7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}