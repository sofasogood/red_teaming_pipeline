{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sofasogood/red_teaming_pipeline/blob/main/Setting_up_model_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbyMmoGt3Qsp"
      },
      "outputs": [],
      "source": [
        "!pip install unsloth\n",
        "!pip install litellm fastapi uvicorn pyngrok accelerate transformers\n",
        "!pip install nest-asyncio\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import os\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "import nest_asyncio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuDdzQ7p3UlC",
        "outputId": "045d6e54-cbee-430d-a719-e88568d3b629"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3bjRbDk3hHx"
      },
      "outputs": [],
      "source": [
        "# Load local sft model\n",
        "\n",
        "# Configure quantization for inference\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# Load base model with quantization\n",
        "base_model_name = \"unsloth/mistral-7b-v0.3-bnb-4bit\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True\n",
        ")\n",
        "\n",
        "# Load LoRA adapter weights\n",
        "adapter_path = \"/content/drive/MyDrive/models/ai_safety/finetuned_model_0131/mistral7b_finetuned/checkpoints/checkpoint-505\"\n",
        "model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    adapter_path,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Optional: Merge LoRA weights with base model for faster inference\n",
        "# model = model.merge_and_unload()\n",
        "\n",
        "# Set eval mode\n",
        "model.eval()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYTwGJY6nh8B"
      },
      "outputs": [],
      "source": [
        "# Importing red_teaming_pipeline\n",
        "!git clone https://github.com/sofasogood/red_teaming_pipeline.git\n",
        "%cd red_teaming_pipeline\n",
        "!pip install -e .\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "U1Qm6WEH6kua"
      },
      "outputs": [],
      "source": [
        "from dataset_creation.data_processing_functions.data_creation_rl import build_instruct_prompt\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "\n",
        "# Input schema\n",
        "class GenerateRequest(BaseModel):\n",
        "    model: str\n",
        "    messages: list[dict]\n",
        "\n",
        "\n",
        "# Output schema\n",
        "class GenerateResponse(BaseModel):\n",
        "    choices: list[str]\n",
        "    model: str\n",
        "\n",
        "@app.post(\"/chat/completions\")\n",
        "async def generate_text(request: GenerateRequest):\n",
        "    prompt = build_instruct_prompt(request.messages)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    prompt_len = inputs[\"input_ids\"].shape[1]\n",
        "    model_name = request.model.strip('openai/')\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=100,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        top_k=50,\n",
        "        repetition_penalty=1.1,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    generated_text = tokenizer.decode(outputs[0][prompt_len:], skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        ")\n",
        "    return {\"choices\": [{\"message\": {\"content\": generated_text}}], \"model\": request.model}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start a port using the terminal:"
      ],
      "metadata": {
        "id": "VRKBBZYqLV61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run these commands\n",
        "\n",
        "1.   `curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh`\n",
        "2.   `. \"$HOME/.cargo/env\"`\n",
        "3. `cargo install bore-cli`\n",
        "4. `bore local 8000 --to bore.pub`"
      ],
      "metadata": {
        "id": "-TZOZazRLyGT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9KWdwDSye-v"
      },
      "outputs": [],
      "source": [
        "# Host model\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNJRZXZOc3PCiVdyzq+sQGG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}